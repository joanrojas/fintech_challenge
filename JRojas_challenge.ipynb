{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fintech Challenge\n",
    "\n",
    "In recent weeks, a fintech company has identified a significant drop of the pass rate in KYC procedure. KYC procedure consists of the following two steps: \n",
    "\n",
    "* Document check: To verify that the photo ID is valid and authentic;\n",
    "* Facial Similarity check: To verify that the face in the picture is the same with that on the submitted ID.\n",
    "\n",
    "I have been granted access to the results (DOCUMENT REPORT -DR- and FACIAL SIMILARITY -FS-) obtained by the two checks for 142.724 users and a total of 176.404 attempts from May to October 2017.\n",
    "\n",
    "In both the DR and FS data sets, each row represents an attempt (unique attempt_id) to pass the checks. \n",
    "\n",
    "Since the pass rate is defined as the number of customers who pass both the KYC process divided by the number of customers who attempt the process, it makes sense to look at a merged dataframe that offers the results and features of both checks - althought first I have had to make sure that the data frames can be properly joined -.\n",
    "\n",
    "In order to understand the magnitude of the issue, I compute the pass rate per week (see .png file, result of plotting the variable 'grouped_results' defined in the code below): \n",
    "\n",
    "<img src=\"files/pass_rate.png\">\n",
    "\n",
    "\n",
    "Clearly, it has been going down since its peak in week 24 where 95,5% of attempts were succesful. While I am not familiar with the business, I believe that a pass rate below 90% is not acceptable. In order to understand the issues that have caused the decline, I will split the data set in two: \n",
    "\n",
    "* **dr_fs_27**: contains all attempts until week 27. \n",
    "* **dr_fs_44**: contains all attempts from week 28 to week 44. \n",
    "\n",
    "By doing the same exploratory work on both data sets and comparing its results, hopefully I'll notice what is the root cause of the decline. \n",
    "\n",
    "### How to use this document \n",
    "\n",
    "The following report contains both the code and the qualitative analysis of the results I obtain, so it combines both code and markdown snippets. The best way to use it is by running all cells. The code will print some intermediate results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "facial_reports = pd.read_csv('facial_similarity_reports.csv')\n",
    "docs_reports = pd.read_csv('doc_reports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------------------\n",
    "#SUPPORT FUNCTIONS\n",
    "#--------------------\n",
    "\n",
    "#Obtain week from date columns into new feature\n",
    "def parse_date(df,date_feature,new_date_feature):\n",
    "    df[date_feature] =  pd.to_datetime(df[date_feature], format='%Y-%m-%dT%H:%M:%S')\n",
    "    df[new_date_feature] = df[date_feature].dt.week\n",
    "    return df\n",
    "\n",
    "#Parse dic-type feature to multiple features and concat datasets\n",
    "def parse_dic(df,dic_type_feature):\n",
    "    df[dic_type_feature] = df[dic_type_feature].apply(lambda x : dict(eval(x)) )\n",
    "    parsed_df = df[dic_type_feature].apply(pd.Series )\n",
    "    df = pd.concat([df, parsed_df], axis=1).drop(dic_type_feature, axis=1)\n",
    "    return df\n",
    "\n",
    "#Return df indexes of observations of feature var that appear more than num_filt times. \n",
    "def rmv_mult_values(df,var,num_filt):\n",
    "    df_s = df.groupby(var).filter(lambda x: len(x) > num_filt)\n",
    "    df_s_indx = df_s.index.values\n",
    "    return df_s, df_s_indx\n",
    "\n",
    "#Check left and inner join of dataframe merges give same results. If check is sucessful, returns merged dataframe.\n",
    "def check_join(df1,df2,join_feature):\n",
    "    df3 = pd.merge(df1, df2, on=join_feature, how='inner')\n",
    "    df4 = pd.merge(df1, df2, on=join_feature, how='left')\n",
    "    if (df3.shape == df4.shape):\n",
    "        return df3\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "#Adds prefix to data sets before joining them.\n",
    "def prep_features_join(df,feature_list,prefix): \n",
    "    pf_feature_list =[prefix + feature for feature in feature_list]\n",
    "    pf_replace = dict(zip(feature_list, pf_feature_list))\n",
    "    df = df.rename(columns = pf_replace)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Return data set df with new feature --> # attempts per user \n",
    "def count_attempts(df,new_feature):\n",
    "    uid_c = df['user_id'].value_counts().rename(new_feature)\n",
    "    df_counts = df.merge(uid_c.to_frame(),left_on='user_id', right_index=True)\n",
    "    return df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Join DR and FS datasets. \n",
    "docs_reports = parse_dic(docs_reports,'properties')\n",
    "\n",
    "#Add DR prefix to its features (only to those that are unique to DR data set)\n",
    "dr_feature_list = ['result', 'visual_authenticity_result',\n",
    "       'image_integrity_result', 'face_detection_result',\n",
    "       'image_quality_result', 'created_at', 'supported_document_result',\n",
    "       'conclusive_document_quality_result', 'colour_picture_result',\n",
    "       'data_validation_result', 'data_consistency_result',\n",
    "       'data_comparison_result', 'police_record_result',\n",
    "       'compromised_document_result', 'properties', 'sub_result']\n",
    "\n",
    "fs_feature_list = ['result', 'face_comparison_result',\n",
    "       'created_at', 'facial_image_integrity_result',\n",
    "       'visual_authenticity_result', 'properties']\n",
    "\n",
    "docs_reports = prep_features_join(docs_reports,dr_feature_list,'DR_')\n",
    "facial_reports = prep_features_join(facial_reports,fs_feature_list,'FS_')\n",
    "\n",
    "#perform merge of data sets\n",
    "merged_dr_fs = pd.merge(docs_reports, facial_reports, on='attempt_id', how='inner')\n",
    "\n",
    "#drop redundant features\n",
    "merged_dr_fs = merged_dr_fs.drop(['user_id_y', 'Unnamed: 0_y'], axis=1)\n",
    "\n",
    "#compute week of the attempt\n",
    "merged_dr_fs = parse_date(merged_dr_fs,'DR_created_at','week_attempt')\n",
    "\n",
    "#compute feature pass_KYC (1 if succesful, 0 if not)\n",
    "conditions = [(merged_dr_fs['DR_result'] == 'clear') & (merged_dr_fs['FS_result'] == 'clear')]\n",
    "val = [1]\n",
    "merged_dr_fs['pass_KYC'] = np.select(conditions, val, default=0)\n",
    "pass_rate_grouped = merged_dr_fs.groupby('week_attempt').agg({'pass_KYC': ['sum','count']})\n",
    "\n",
    "#group results by partial checks DOCUMENT and FACIAL\n",
    "grouped_results = merged_dr_fs.groupby(['DR_result', 'FS_result']).count()['attempt_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................\n",
      "SEGMENT WEEK 21 - 27 ATTEMPTS BY RESULT AND SUB_RESULT\n",
      "..........................................................................\n",
      "DR_result  FS_result\n",
      "clear      clear        15319\n",
      "           consider       987\n",
      "consider   clear          522\n",
      "           consider        77\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "SEGMENT WEEK 28 - 44 ATTEMPTS BY RESULT AND SUB_RESULT\n",
      "..........................................................................\n",
      "DR_result  FS_result\n",
      "clear      clear        111570\n",
      "           consider       4524\n",
      "consider   clear         38074\n",
      "           consider       5329\n",
      "Name: attempt_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Compute two data sets\n",
    "# dr_fs_27 --> attempts before week 28 \n",
    "# dr_fs_44 --> attempts after week 28 \n",
    "dr_fs_27 = merged_dr_fs.loc[merged_dr_fs['week_attempt'] <= 27]\n",
    "dr_fs_44 = merged_dr_fs.loc[merged_dr_fs['week_attempt'] > 27]\n",
    "print('..........................................................................')\n",
    "print('SEGMENT WEEK 21 - 27 ATTEMPTS BY RESULT AND SUB_RESULT')\n",
    "print('..........................................................................')\n",
    "print(dr_fs_27.groupby(['DR_result', 'FS_result']).count()['attempt_id'])\n",
    "print('..........................................................................')\n",
    "print('SEGMENT WEEK 28 - 44 ATTEMPTS BY RESULT AND SUB_RESULT')\n",
    "print('..........................................................................')\n",
    "print(dr_fs_44.groupby(['DR_result', 'FS_result']).count()['attempt_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of these results, it looks like there's a fundamental shift as to why attempts fail between the two data sets. In the \"healthy\" data set, the fail rate (1 - pass_rate) is rougly 9%; of which 6% comes from passing the document check but not the facial similarity check. \n",
    "\n",
    "On the other hand, in the \"damaged\" data set, the driver of the fail rate is not a fail in the facial similarity check but in the document check. In particular, the fail rate is 30%, of which 24% comes from not passing the document check.  Clearly this is a significant issue that needs to be adressed. Rougly one fourth of new users cannot finish the on-boarding process correctly hence Revolut's user acquisition activity is damaged and so is its user experience. \n",
    "\n",
    "Given this new discovery, it makes sense to focus on the DR dataset. Hopefully, a thorough analysis of it will shed light on why there are so many failed attempts. \n",
    "\n",
    "As per the Veritas API documentation, DOCUMENT REPORT presents two differnet result features, namely: \n",
    "* result: The overall result of the check, can take two values - 'consider' / 'clear'\n",
    "* sub_result, which indicates a more detailed result. It can take the following values: \n",
    "    * **clear**\tIf all underlying verifications pass, the overall sub result will be clear\n",
    "    * **rejected**\tIf the report has returned information where the check cannot be processed further (poor quality image or an unsupported document).\n",
    "    * **suspected**\tIf the document that is analysed is suspected to be fraudulent.\n",
    "    * **caution** if any other underlying verifications fail but they don’t necessarily point to a fraudulent document (such as the name provided by the applicant doesn’t match the one on the document)\n",
    "    \n",
    "By segmenting the attempts of the 'damaged' data set  by result and sub_result, it looks like most unsuccesful attempts report 'caution' or 'rejected' in sub_result (15.469 and 26.071 failed attempts, respectively). It might be worth to analyse them separately.\n",
    "\n",
    "Following with the API documentation,  the DOCUMENT REPORT is composed of data integrity, visual authenticity and police record checks. It checks the internal and external consistency of the most recent identity document provided by the applicant to identify potential discrepancies.\n",
    "\n",
    "Although not stated explicitly, it looks like the result - feature 'result' -  depends on the results of simpler checks, referenced in the following features: \n",
    "\n",
    "* Visual authenticity\n",
    "* Image integrity\n",
    "    * Face detection\n",
    "    * Image quality\n",
    "    * Supported document\n",
    "    * Conclusive document quality\n",
    "    * Colour picture\n",
    "* Data validation\n",
    "* Data consistency\n",
    "* Data comparison\n",
    "* Police record\n",
    "* Compromised document\n",
    "\n",
    "A simple exploratory analysis shows that most of these features take one of the following values: NaN (empty field), 'clear' or 'consider'. Additionally, it looks like image integrity fields - image quality and supported document - also take the value 'unidentified'. \n",
    "\n",
    "The Veritas API does not seem to refer to the meaning of NaN values in these features. It would be worth exploring about these empty fields - and more importantly, investigate why they are not filled correctly.  Let's leave this for later. \n",
    "\n",
    "Perhaps what is more relevant is that in almost 40.000 attempts, the feature referring to the image integrity test was labelled as 'consider' , meaning it didn't pass the check. The image integrity check seems to depends on the partial checks regarding face detection, image quality, supported document, conclusive document quality and colour picture so it's worth investigating these further. \n",
    "\n",
    "At this point I have enough data to state a hypothesis - most of the decrease in failing rate is due to a failure in passing the image integrity sub check, which is part of the document check. \n",
    "\n",
    "In the 'damaged' data set, there seems to be a significant increase in 'rejected' attempts as per the value of the feature sub_result. According to the API documentation, the feature takes this value if the report has returned information where the check cannot be processed further (poor quality image or an unsupported document). This information relates very well to the hypothesis I stated above. \n",
    "\n",
    "In fact, after removing all rows in the damaged data set where the image integrity result is NOT 'consider', I end up with a set that does not contain any 'rejected' attempts (with some 'caution' and 'consider'), so it seems there is a direct cause effect between attempts with poor image integrity and those that end up in 'rejected'. Plus, since most of the failed attempts have sub_result 'rejected', it makes sense to focus those attempts first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................\n",
      "WEEK 21 - 27 DATA SET: SEGMENT ATTEMPTS BY RESULT AND SUB_RESULT\n",
      "..........................................................................\n",
      "DR_result  DR_sub_result\n",
      "clear      clear            16306\n",
      "consider   caution            523\n",
      "           rejected            19\n",
      "           suspected           57\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "WEEK 28 - 44 DATA SET: SEGMENT ATTEMPTS BY RESULT AND SUB_RESULT\n",
      "..........................................................................\n",
      "DR_result  DR_sub_result\n",
      "clear      clear            116095\n",
      "consider   caution           15469\n",
      "           rejected          26071\n",
      "           suspected          1863\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_visual_authenticity_result\n",
      "..........................................................................\n",
      "DR_visual_authenticity_result\n",
      "clear       130634\n",
      "consider      2769\n",
      "nan          26095\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_image_integrity_result\n",
      "..........................................................................\n",
      "DR_image_integrity_result\n",
      "clear       119773\n",
      "consider     39724\n",
      "nan              1\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_data_validation_result\n",
      "..........................................................................\n",
      "DR_data_validation_result\n",
      "clear       125588\n",
      "consider      1438\n",
      "nan          32472\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_data_consistency_result\n",
      "..........................................................................\n",
      "DR_data_consistency_result\n",
      "clear       82459\n",
      "consider      151\n",
      "nan         76888\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_data_comparison_result\n",
      "..........................................................................\n",
      "DR_data_comparison_result\n",
      "nan    159498\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_police_record_result\n",
      "..........................................................................\n",
      "DR_police_record_result\n",
      "clear       129167\n",
      "consider        24\n",
      "nan          30307\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_compromised_document_result\n",
      "..........................................................................\n",
      "DR_compromised_document_result\n",
      "clear        45483\n",
      "consider        23\n",
      "nan         113992\n",
      "Name: attempt_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Segment attempts by result and sub_result - clearly there is a significant increase in 'rejected' results. \n",
    "print('..........................................................................')\n",
    "print('WEEK 21 - 27 DATA SET: SEGMENT ATTEMPTS BY RESULT AND SUB_RESULT')\n",
    "print('..........................................................................')\n",
    "print(dr_fs_27.groupby(['DR_result','DR_sub_result']).count()['attempt_id'])\n",
    "print('..........................................................................')\n",
    "print('WEEK 28 - 44 DATA SET: SEGMENT ATTEMPTS BY RESULT AND SUB_RESULT')\n",
    "print('..........................................................................')\n",
    "print(dr_fs_44.groupby(['DR_result','DR_sub_result']).count()['attempt_id'])\n",
    "\n",
    "\n",
    "f = ['DR_visual_authenticity_result',\n",
    "       'DR_image_integrity_result', 'DR_data_validation_result',\n",
    "       'DR_data_consistency_result', 'DR_data_comparison_result',\n",
    "       'DR_police_record_result', 'DR_compromised_document_result']\n",
    "\n",
    "for element in f: \n",
    "    print('..........................................................................')\n",
    "    print('NUMBER OF ATTEMPTS GROUPED BY' +' ' + element)\n",
    "    print('..........................................................................')\n",
    "    print(dr_fs_44.astype(str).groupby([element]).count()['attempt_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................\n",
      "SEGMENT ATTEMPTS WHERE IMAGE_INTEGRITY <> CONSIDER;  BY RESULT AND SUB_RESULT\n",
      "..........................................................................\n",
      "DR_result  DR_sub_result\n",
      "clear      clear            116095\n",
      "consider   caution            2031\n",
      "           suspected          1648\n",
      "Name: attempt_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_hypothesis = dr_fs_44.loc[dr_fs_44['DR_image_integrity_result'] != 'consider']\n",
    "print('..........................................................................')\n",
    "print('SEGMENT ATTEMPTS WHERE IMAGE_INTEGRITY <> CONSIDER;  BY RESULT AND SUB_RESULT')\n",
    "print('..........................................................................')\n",
    "print(test_hypothesis.groupby(['DR_result','DR_sub_result']).count()['attempt_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS WITH FAILURE IN IMAGE INTEGRITY OVER THE TIME PERIOD\n",
      "..........................................................................\n",
      "             pass_KYC\n",
      "                count\n",
      "week_attempt         \n",
      "23                  2\n",
      "24                  2\n",
      "25                  4\n",
      "26                  7\n",
      "27                  4\n",
      "28                713\n",
      "29                887\n",
      "30               1055\n",
      "31               1027\n",
      "32               1009\n",
      "33               1160\n",
      "34               1128\n",
      "35               1260\n",
      "36               1219\n",
      "37               1900\n",
      "38               3003\n",
      "39               3398\n",
      "40               6169\n",
      "41               6989\n",
      "42               3056\n",
      "43               4992\n",
      "44                759\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_image_quality_result\n",
      "..........................................................................\n",
      "DR_image_quality_result\n",
      "clear           15330\n",
      "unidentified    24394\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_supported_document_result\n",
      "..........................................................................\n",
      "DR_supported_document_result\n",
      "clear           37546\n",
      "nan               501\n",
      "unidentified     1677\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_conclusive_document_quality_result\n",
      "..........................................................................\n",
      "DR_conclusive_document_quality_result\n",
      "clear          88\n",
      "consider    13565\n",
      "nan         26071\n",
      "Name: attempt_id, dtype: int64\n",
      "..........................................................................\n",
      "NUMBER OF ATTEMPTS GROUPED BY DR_colour_picture_result\n",
      "..........................................................................\n",
      "DR_colour_picture_result\n",
      "clear       13549\n",
      "consider      104\n",
      "nan         26071\n",
      "Name: attempt_id, dtype: int64\n",
      "DR_image_quality_result\n",
      "clear            1677\n",
      "unidentified    24394\n",
      "Name: attempt_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#DEFINE NEW DATA SET dr_fs_iif; FILTER merged_dr_fs TO KEEP ONLY ATTEPMTS WHERE image_integrity_result == 'consider' \n",
    "\n",
    "#Group attempts with image_integrity_result = 'consider' by week\n",
    "#Clearly, the issue starts to appear in week 28. \n",
    "dr_fs_iif = merged_dr_fs.loc[merged_dr_fs['DR_image_integrity_result'] == 'consider']\n",
    "print('..........................................................................')\n",
    "print('NUMBER OF ATTEMPTS WITH FAILURE IN IMAGE INTEGRITY OVER THE TIME PERIOD')\n",
    "print('..........................................................................')\n",
    "print(dr_fs_iif.groupby('week_attempt').agg({'pass_KYC': ['count']}))\n",
    "\n",
    "#Group attempts with image_integrity_result = 'consider' in the damaged data set\n",
    "dr_fs_44_iif = dr_fs_44.loc[dr_fs_44['DR_image_integrity_result'] == 'consider']\n",
    "\n",
    "\n",
    "#Analysis of image integrity sub features \n",
    "f_b = ['DR_image_quality_result',\n",
    "       'DR_supported_document_result', 'DR_conclusive_document_quality_result',\n",
    "       'DR_colour_picture_result']\n",
    "\n",
    "for element in f_b: \n",
    "    print('..........................................................................')\n",
    "    print('NUMBER OF ATTEMPTS GROUPED BY' +' ' + element)\n",
    "    print('..........................................................................')\n",
    "    print(dr_fs_44_iif.astype(str).groupby([element]).count()['attempt_id'])    \n",
    "\n",
    "a = dr_fs_44_iif[dr_fs_44_iif['DR_conclusive_document_quality_result'].isnull()]\n",
    "print(a.astype(str).groupby(['DR_image_quality_result']).count()['attempt_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and further work \n",
    "\n",
    "On-boarding pass rate for new users has decreased significantly in recent weeks. From week 21 to 27, pass rate remained above 90%, and since week 28 has decreased bottoming at 55% in week 41. Comparing both data sets, it's clear there's a fundamental shift as to why attempts fail. In the \"healthy\" data set, the fail rate (1 - pass_rate) is rougly 9%; of which 6% comes from passing the document check but not the facial similarity check, whereas in the \"damaged\" data set, the driver of the fail rate is not a fail in the facial similarity check but in the document check. In particular, the fail rate is 30%, of which 24% comes from not passing the document check.\n",
    "\n",
    "At a closer look, the document check's fail rate is around 27% (43.403 attempts). 60% of these attempts are labelled as rejected, 35% as caution and 5% as suspected.\n",
    "\n",
    "\n",
    "Rejected attempts are due to an issue around the integrity of the picture of the document (picture of the passport, national ID or driving license) uploaded for the document check. In particular, it seems like the picture of the document is not read properly due to supposedly bad quality.  \n",
    "\n",
    "Before exploring solutions, first I would manually, if possible, try to find further data that reinforces this conclusion. For instance, we could potentially explore the look&feel of some of the damaged pictures. This would allow us to have a better understanding of the problem. It could potentially help answer questions like: is it our software that is not working properly? Or is it the users that do not understand how should the picture be taken? Another piece of data that would reinforce this conclusion would be to look at the topic of tickets opened in customer support. \n",
    "\n",
    "In order to address this issue, there are few areas we could potentially take action on. Regarding product, we could review the on-boarding process and in-screen help to ensure the user is aware how should she take the picture. \n",
    "We could also potentially include some sort of photo-filter to improve its quality before sending it to Veritas for verification. \n",
    "\n",
    "\n",
    "### Further work \n",
    "\n",
    "While this analysis addresses the 60% of the failed attempts in Document check, I would continue my analysis to understand why there are so many attempts labelled as 'caution'. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
